{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be doing **4 exercises** to become familar with some of the bioinformatic tools that are used by researchers today. These tools are currently being used actively by researchers who are studying microbial genomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning how to create Conda environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While conda is highly useful for installing various bioinformatic tools, sometimes some tools are not compatible with each other. This can \"break\" your Conda environment if you try to install 2 tools that may not be compatible. It's difficult to predict which tools may or may not be compatible with each other but sometimes you discover this by accident. Such is the case with `sra-tools` that we were trying to install last week.\n",
    "\n",
    "So here, I will show you how to install this tool by itself in a contained Conda environment that you can remove if the tool isn't working the way it should. So follow the steps below to create a Conda environment for this tool.\n",
    "\n",
    "This command below creates a conda environment, which we are going to name as \"sra-tools\"\n",
    "\n",
    "`conda create -n sra-tools`\n",
    "\n",
    "Then you will have to \"activate\" it, by typing:\n",
    "\n",
    "`conda activate sra-tools`\n",
    "\n",
    "Then you will have to search for the sra-tools and install a specific version of it. What broke the installation last time was that a default \"install\" command installed a deprecated (old) version of the tool from Conda repositories. So this time, we'll install a specific version. Type:\n",
    "\n",
    "`conda install sra-tools=3.2.1`\n",
    "\n",
    "In this command above, you are telling Conda to install `sra-tools` version 3.2.1, which is the latest stable version available in Bioconda. Once this is done, you should have various commands that are part of the `sra-tools` kit. Instructions on how to use `sra-tools` are shown towards the end of the exercises for today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sanger sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will learn how to inspect and read a Sanger chromatogram. Download the 3 sequences provided as examples here:\n",
    "\n",
    "https://gwu.box.com/s/vvxpy5s4rjrzmqvm4jqussiyj2xgqfra (You need to have a GW Box account to access the contents here)\n",
    "\n",
    "These are what we call \"raw\" Sanger reads with extension name `\".ab1\"`. To view the chromatograms, you will need to download specific tools designed to view them. \n",
    "\n",
    "If you have a Mac computer, you can download **4peaks** here to view the chromatograms. See here: https://nucleobytes.com/4peaks/\n",
    "\n",
    "If you have a Windows computer, you can use [Chromas Lite](http://technelysium.com.au/wp/chromas/), [FinchTV](https://digitalworldbiology.com/FinchTV), or [BioEdit](https://bioedit.software.informer.com/7.2/).\n",
    "\n",
    "A more comprehensive list of viewers you can use can be found here: https://www.genewiz.com/en/Public/Resources/Tools-for-Viewing-Sequencing-Data\n",
    "\n",
    "Once you have a viewing tool installed on your computer, the `.ab1` files will become viewable using those tools. Open the files and see what they look like. They all contain chromatograms showing various peaks of DNA letters (A, C, G, and T). See if you can differentiate between a good Sanger sequencing read looks like and if you can determine what information is contained within the sequences.\n",
    "\n",
    "Try copying the sequences and running a BLAST search on NCBI. https://blast.ncbi.nlm.nih.gov/Blast.cgi\n",
    "\n",
    "Can you tell what these sequences encode? Which one of them is the best in terms of sequencing quality? Which one is the worst?\n",
    "\n",
    "Now take a look at their corresponding `.phd.1` files in the Box folder shared with you. These are simply **text files** that you can open with a text editor (such as Notepad or Wordpad). Take a look and see what they contain. \n",
    "\n",
    "An example few lines of one of these files show this:\n",
    "\n",
    "```\n",
    "BEGIN_SEQUENCE BE1a-515F_A07\n",
    "\n",
    "BEGIN_COMMENT\n",
    "\n",
    "CHROMAT_FILE: BE1a-515F_A07.ab1\n",
    "BASECALLER_VERSION: KB 1.2\n",
    "TRACE_PROCESSOR_VERSION: KB 1.2\n",
    "QUALITY_LEVELS: 99\n",
    "TIME: Fri Oct 29 00:39:41 2021\n",
    "TRACE_ARRAY_MIN_INDEX: 0\n",
    "TRACE_ARRAY_MAX_INDEX: 11005\n",
    "TRIM: -1 -1 -1.000000e+000\n",
    "TRACE_PEAK_AREA_RATIO: -1.000000e+000\n",
    "CHEM: term\n",
    "DYE: big\n",
    "\n",
    "END_COMMENT\n",
    "\n",
    "BEGIN_DNA\n",
    "N 3 3\n",
    "N 1 9\n",
    "N 2 54\n",
    "N 4 62\n",
    "N 4 72\n",
    "N 3 81\n",
    "N 3 112\n",
    "N 4 120\n",
    "N 4 132\n",
    "N 7 149\n",
    "N 6 167\n",
    "N 6 181\n",
    "N 4 197\n",
    "N 5 211\n",
    "N 7 232\n",
    "N 6 253\n",
    "N 4 270\n",
    "N 4 282\n",
    "N 5 291\n",
    "N 9 300\n",
    "N 5 313\n",
    "N 4 320\n",
    "N 4 345\n",
    "N 6 364\n",
    "N 6 374\n",
    "N 6 395\n",
    "N 8 411\n",
    "N 5 427\n",
    "N 7 436\n",
    "T 31 447\n",
    "\n",
    "```\n",
    "\n",
    "and they end like this:\n",
    "\n",
    "```\n",
    "C 16 10659\n",
    "G 21 10671\n",
    "A 25 10684\n",
    "N 7 10694\n",
    "T 18 10703\n",
    "T 25 10714\n",
    "N 9 10729\n",
    "T 10 10738\n",
    "T 28 10749\n",
    "T 31 10761\n",
    "T 28 10773\n",
    "C 19 10783\n",
    "A 12 10793\n",
    "C 22 10804\n",
    "C 25 10814\n",
    "T 23 10827\n",
    "T 27 10839\n",
    "G 28 10850\n",
    "G 23 10862\n",
    "N 8 10875\n",
    "C 11 10885\n",
    "N 8 10900\n",
    "N 5 10913\n",
    "N 7 10927\n",
    "N 9 10936\n",
    "G 14 10946\n",
    "G 33 10956\n",
    "N 9 10967\n",
    "N 6 10978\n",
    "N 7 10991\n",
    "END_DNA\n",
    "\n",
    "END_SEQUENCE\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, they encode approximate positions (3rd column) of the DNA bases identified (1st column) and their associated quality scores (2nd column). The values in the 2nd column range from 0 to 99 and they are called [\"Phred scores\"](https://en.wikipedia.org/wiki/Phred_quality_score). So this file essentially contains chromatogram read quality information in a text format. Earlier bioinformaticians had to write computational tools to convert these traces into `.fasta` and `.qual` files that can later be used as input files for various bioinformatic tools. What can you tell by comparing these `.phd.1` files of these 3 example sequences? Anything jumps out at you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. sra-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is sra-tools, which are a suite of tools that you can use to download data from NCBI sequence read archives (SRA) (see here: https://www.ncbi.nlm.nih.gov/sra). This database is essential to a lot of researchers. This is where they will deposit raw sequences that came out of sequencing instruments. You can search for research project focusing on a certain topic and download the data they have deposited to SRA. It is publicly accessible. \n",
    "\n",
    "For example, if you type \"Yellowstone hot spring\" in the search box, you will see a list of accessions with links to the data. An example is here:\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/sra/SRX9071323\n",
    "\n",
    "If you click on that link, you will see that the study is titled \"Seasonal hydrologic and geologic forcing drive hot spring geochemistry and microbial biodiversity\". And there is a link below that says \"Run, # of spots, # of Bases\" etc. To download this sequence data, you will need to click on the link that starts with \"SRR\", click on the \"Data access\" tab, then click on the link provided next to the \"run\". \n",
    "\n",
    "So it becomes very cumbersome and difficult to navigate these pages to access and download the data. This is where the sra-tools becomes very useful. You can download the raw sequence reads deposited to SRA by just typing the following command with the accession number next to the `[accn]`.\n",
    "\n",
    "```\n",
    "fasterq-dump --split-3 SRR12584454\n",
    "```\n",
    "\n",
    "The **\"SRR12584454\"** is the actual run accession number for that particular study. This is actually the number you need to enter when trying to download these raw sequences from NCBI SRA.\n",
    "\n",
    "First, you might want to explore what `fasterq-dump` means by typing `fasterq-dump -h`. `-h` just means you are trying to bring up help menu to list the options available with this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:\n",
      "  fasterq-dump <path> [options]\n",
      "  fasterq-dump <accession> [options]\n",
      "\n",
      "Options:\n",
      "  -F|--format                      format (special, fastq, default=fastq) \n",
      "  -o|--outfile                     output-file \n",
      "  -O|--outdir                      output-dir \n",
      "  -b|--bufsize                     size of file-buffer dflt=1MB \n",
      "  -c|--curcache                    size of cursor-cache dflt=10MB \n",
      "  -m|--mem                         memory limit for sorting dflt=100MB \n",
      "  -t|--temp                        where to put temp. files dflt=curr dir \n",
      "  -e|--threads                     how many thread dflt=6 \n",
      "  -p|--progress                    show progress \n",
      "  -x|--details                     print details \n",
      "  -s|--split-spot                  split spots into reads \n",
      "  -S|--split-files                 write reads into different files \n",
      "  -3|--split-3                     writes single reads in special file \n",
      "  --concatenate-reads              writes whole spots into one file \n",
      "  -Z|--stdout                      print output to stdout \n",
      "  -f|--force                       force to overwrite existing file(s) \n",
      "  --skip-technical                 skip technical reads \n",
      "  --include-technical              include technical reads \n",
      "  -M|--min-read-len                filter by sequence-len \n",
      "  --table                          which seq-table to use in case of pacbio \n",
      "  -B|--bases                       filter by bases \n",
      "  -A|--append                      append to output-file \n",
      "  --fasta                          produce FASTA output \n",
      "  --fasta-unsorted                 produce FASTA output, unsorted \n",
      "  --fasta-ref-tbl                  produce FASTA output from REFERENCE tbl \n",
      "  --fasta-concat-all               concatenate all rows and produce FASTA \n",
      "  --internal-ref                   extract only internal REFERENCEs \n",
      "  --external-ref                   extract only external REFERENCEs \n",
      "  --ref-name                       extract only these REFERENCEs \n",
      "  --ref-report                     enumerate references \n",
      "  --use-name                       print name instead of seq-id \n",
      "  --seq-defline                    custom defline for sequence:  $ac=accession, \n",
      "                                   $sn=spot-name,  $sg=spot-group, $si=spot-id,  \n",
      "                                   $ri=read-id, $rl=read-length \n",
      "  --qual-defline                   custom defline for qualities:  same as \n",
      "                                   seq-defline \n",
      "  -U|--only-unaligned              process only unaligned reads \n",
      "  -a|--only-aligned                process only aligned reads \n",
      "  --disk-limit                     explicitly set disk-limit \n",
      "  --disk-limit-tmp                 explicitly set disk-limit for temp. files \n",
      "  --size-check                     switch to control: on=perform size-check \n",
      "                                   (default),  off=do not perform size-check,  \n",
      "                                   only=perform size-check only \n",
      "  --ngc <PATH>                     PATH to ngc file \n",
      "\n",
      "  -h|--help                        Output brief explanation for the program. \n",
      "  -V|--version                     Display the version of the program then \n",
      "                                   quit. \n",
      "  -L|--log-level <level>           Logging level as number or enum string. One \n",
      "                                   of (fatal|sys|int|err|warn|info|debug) or \n",
      "                                   (0-6) Current/default is warn. \n",
      "  -v|--verbose                     Increase the verbosity of the program \n",
      "                                   status messages. Use multiple times for more \n",
      "                                   verbosity. Negates quiet. \n",
      "  -q|--quiet                       Turn off all status messages for the \n",
      "                                   program. Negated by verbose. \n",
      "  --option-file <file>             Read more options and parameters from the \n",
      "                                   file. \n",
      "for more information visit:\n",
      "   https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n",
      "   https://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump\n",
      "\n",
      "fasterq-dump : 3.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fasterq-dump -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are multiple options available with this tool and you can even specify things like memory limitation for this command to run and the number of threads you want to use with this tool. Before actually typing the command, I want you to first organize your folders associated with this course to look a few basic things. You should first create a main folder for the course, which I suggest you name `MicrobialGenomicsLab`. Follow the few commands below.\n",
    "\n",
    "```bash\n",
    "cd\n",
    "mkdir MicrobialGenomicsLab\n",
    "cd MicrobialGenomicsLab\n",
    "mkdir data docs exercises tools\n",
    "```\n",
    "\n",
    "Then change your directory to \"exercises\" directory.\n",
    "\n",
    "```bash\n",
    "cd exercises\n",
    "```\n",
    "\n",
    "The folders should look something like this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mMicrobialGenomicsLab\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "├── \u001b[01;34mdocs\u001b[0m\n",
      "├── \u001b[01;34mexercises\u001b[0m\n",
      "└── \u001b[01;34mtools\u001b[0m\n",
      "\n",
      "5 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd\n",
    "tree MicrobialGenomicsLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will see that I created the 4 folders inside \"MicrobialGenomicsLab\" folder which resides in my home folder. Navigate into the exercises folder and run the `fasterq-dump` tool with the example command that I wrote earlier. Make sure you have activated the `sra-tools` Conda environment first.\n",
    "\n",
    "```bash\n",
    "conda activate sra-tools\n",
    "fasterq-dump --split-3 SRR12584454\n",
    "```\n",
    "\n",
    "The command will start downloading the raw sequence files from SRA to your \"exercises\" folder. It may take a few minutes and you will see the report printed to the screen once it's done. Usually it will not tell you what is going on but you can increase the verbosity of the screen output by typing like this:\n",
    "\n",
    "```bash\n",
    "fasterq-dump -v --split-3 SRR12584454\n",
    "```\n",
    "\n",
    "Once it's done, you can type `ls -la` to see what files the tool produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 331776\n",
      "-rw-r--r--  1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_1.fastq\n",
      "-rw-r--r--  1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_2.fastq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/MicrobialGenomicsLab/exercises/\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see 2 files being produced by the `fasterq-dump` tool. Both files end with a \".fastq\" file extension. These are fastq-formatted files that can be observed/analyzed with tools like `fastqc` or `bbmap`. Now, try to see what the contents of these fastq files look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SRR12584454.1 1 length=301\n",
      "GTGCCAGCCGCCGCGGTAATACCAGCCCCGCGAGTGGTCGGGACTCTTACTGGGCCTAAAGCGCCCGTAGCCGGCCCGACAAGTCACTCCTTAAAGACCCCGGCTCAACCGGGGGAATGGGGGTGATACTGTCGGGCTAGGGGGCGGAAGAGGCCAGCGGTACTCCCGGAGTAGGGGCGAAATCCTCAGATCCCGGGAGGACCACCAGTGGCGAAAGCGGCTGGCTAGAACGCGCCCGACGGTGGGGGGCGAAAGGCGGGGCAGAGAAAGGGATTAGAAAACCCTTGAGGTCAGATGGGAA\n",
      "+SRR12584454.1 1 length=301\n",
      "CCCCCGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG@FCFGGGGGGGGGGGGGGGGGGGAFGGGGGGGEGGCCGGGGGCCGGGGGGGGGECGCFFGGGGGGGEFGGGGGGGGC6:CGGCGEEGGGGGGGGGG=:8EEC6C:C:?C*2<:<A<959*:763**:,;)/:EG(1):*-(2/><C@)-((.<F483((2))0:9,;855*.-*((,)(--)+20+\n",
      "@SRR12584454.2 2 length=301\n",
      "GTGTCAGCAGCCGCGGTAATACGGAGGGTGCGAGCGTTACTCGGAATTACTGGGCGTAAAGCGCGCGTAGGTGGTTTGTTAAGTTGGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATTCCAAACTGACGAGCTAGAGTATGGTAGAGGGTGGTGGAATTTCCTGTGTAGCGGTGAAATGCGTAGATATAGGAAGGAACACCAGTGGCGAAGGCGACCACCTGGAATGATACTGACAGTGAGGTGCGAAAGCGGGGGGAGCAAACAGGATTAGATACCCCGGTAGTCCAGATCGGAA\n",
      "+SRR12584454.2 2 length=301\n",
      "CCCCCGGGAFGGGGGGGGGGGG,6FGG788C+CFGGGG7,9EF,+6CFGGGGFGGGGGGGGGG7+@FGGG,:EDGG844BFGG<,F,,CFG9FFGCGGG+++@FGGGGG3F,BF<F+=FFGGGG,,@FGGG,3>++@F3DFGG,=3FG9,@FG:3CF>FGGGGG,FG,?F<FFFGGGGGGGGGGGGGGGGGGGCFG,BFGGGCFGGFGGGFEGGGGG=G*?E8*2CFG6<+97CCEGD?E9<<7>CGC5:D3C>1:/3>DDF@118?C*7;C?DFB<C*@7CC)014(/8)294<>?EAE5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/MicrobialGenomicsLab/exercises\n",
    "head -8 SRR12584454_1.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I typed `head -8 SRR12584454_1.fastq` to print the first 8 lines of the fastq file to the terminal screen. As you can see, the file stores a string of letters representing the bases of DNA and other weird characters. Try to see if you can see a pattern here. Each record for a single fragment of DNA is represented by 4 lines. \n",
    "\n",
    "The first line starts with a '@' followed by a string of alphabets and numbers. This should be unique to each sequence record. The actual DNA sequence is on the 2nd line. The third line starts with '+' and same identifier as the first line. The 4th line contains characters that represent sequence quality for each of the DNA bases shown on the 2nd line. This is very important information for tools like `fastqc` or `bbmap` that relies on this information to assess sequence quality.\n",
    "\n",
    "To understand more about how fastq files are encoded, see here: https://en.wikipedia.org/wiki/FASTQ_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see 2 fastq files that end with \"\\_1\" and \"\\_2\" in their file names. The reason for that is Illumina sequences that produced these files is usually run to sequence two ends of a single DNA fragment and for each DNA fragment that was sequenced, you have two sequences that you know are physically linked and therefore we call them **\"read pairs\"**. Pairing information is very useful for genome assembly and mapping. We will come to that in later labs.\n",
    "\n",
    "Now that you have **2 fastq files**, what do we do with them? First, what would you do if you want to know how many sequences are in these files? You can type something like this:\n",
    "\n",
    "```bash\n",
    "grep -c \"@SRR12584454\" SRR12584454_1.fastq\n",
    "grep -c \"@SRR12584454\" SRR12584454_2.fastq\n",
    "```\n",
    "\n",
    "And both should return the same numbers. I will show you the example below. These are relatively small files by Illumina sequencing standards (only about 67 Mbp) so it is ok to run this `grep` command. I would not recommend this with files larger than several gigabytes large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103842\n",
      "103842\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/workspace/MicrobialGenomicsLab/exercises/\n",
    "grep -c \"@SRR12584454\" SRR12584454_1.fastq\n",
    "grep -c \"@SRR12584454\" SRR12584454_2.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I am searching for the pattern \"@SRR12584454\" in each fastq file because I know that this identifier is present in each of the header line of the sequences. `grep -c` just counts the occurrences of these identifiers instead of printing the matches to screen. I see that both fastq files contain 103,842 sequences. This is very small number. Usually you will have millions of sequences per fastq file. So there is no way for you to manually inspect each and every one of these sequences for their quality. This is where the FastQC tool comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `ffq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ffq` is another tool that allows you to query resources from SRA to download them directly to your local environment without having to point and click through a web browser. You can install it using the `conda` command.\n",
    "\n",
    "First, install it as usual using conda.\n",
    "\n",
    "```\n",
    "conda install ffq=0.3.1\n",
    "```\n",
    "\n",
    "It should install quite easily without having too much issues. After that, test to see if it is running correctly by typing `ffq -h`\n",
    "\n",
    "It should display a bunch of helpful hints and options to run the program. It does not display all the help or examples but if you want to explore what it can do, please check out their Github page here: https://github.com/pachterlab/ffq\n",
    "\n",
    "After you have installed the tool correctly, you can test some of the examples shown in the Github page. Basically, it retrieves important metadata associated with various sequencing runs or projects deposited to NCBI SRA, EBI ENA, DDBJ, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `ffq`, let's try how we can download the raw data from SRA accession number SRR12584454. \n",
    "\n",
    "Type `ffq SRR12584454` in your terminal. This will display a bunch of information including the study title, accession number, and other attributes such as when the data was first became public, etc. Next, we are going to find direct urls to download the raw fastq files associated with the accession number. \n",
    "\n",
    "Type `ffq --ftp SRR12584454` and you will see that it displays urls for both the forward and reverse fastq files of the accession number. I will display below what it actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-26 02:34:22,416]    INFO Parsing run SRR12584454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"accession\": \"SRR12584454\",\n",
      "        \"filename\": \"SRR12584454_1.fastq.gz\",\n",
      "        \"filetype\": \"fastq\",\n",
      "        \"filesize\": 12858209,\n",
      "        \"filenumber\": 1,\n",
      "        \"md5\": \"bf79f9f40c14bbd2ed59ce9ae1b96205\",\n",
      "        \"urltype\": \"ftp\",\n",
      "        \"url\": \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/054/SRR12584454/SRR12584454_1.fastq.gz\"\n",
      "    },\n",
      "    {\n",
      "        \"accession\": \"SRR12584454\",\n",
      "        \"filename\": \"SRR12584454_2.fastq.gz\",\n",
      "        \"filetype\": \"fastq\",\n",
      "        \"filesize\": 17418735,\n",
      "        \"filenumber\": 2,\n",
      "        \"md5\": \"a6be2baa7d5853523f2e11e057fea251\",\n",
      "        \"urltype\": \"ftp\",\n",
      "        \"url\": \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/054/SRR12584454/SRR12584454_2.fastq.gz\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ffq --ftp SRR12584454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives you the direct url links to the gzipped fastq files that you can download using the `wget` command in Unix/Linux.\n",
    "\n",
    "You can download the fastq files like this:\n",
    "\n",
    "```bash\n",
    "wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/054/SRR12584454/SRR12584454_1.fastq.gz\n",
    "wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/054/SRR12584454/SRR12584454_2.fastq.gz\n",
    "\n",
    "```\n",
    "\n",
    "If you have already downloaded them using the `fasterq-dump` command, you don't have to download them again using the `wget` command.\n",
    "\n",
    "Now, you have the two fastq files needed for the steps 3 and 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FastQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC tool can be found here: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ but also through `conda` and you should have installed it through `conda` by now. If you haven't already installed it, just use the `conda` command like this: `conda install fastqc`.\n",
    "\n",
    "It is a tool to analyze high-throughput sequencing data such as those produced by Illumina sequencing technology. It exists in both graphical and command line modes. After installation, if you just type `fastqc` without any parameters, it will bring up the graphical interface. For bioinformaticians, however, we like to work in command line mode as much as possible due to large number of files that we usually need to process in automated fashion. Today, we will use fastqc to inspect the sequence quality of these 2 files you just downloaded from SRA.\n",
    "\n",
    "In Unix environment, using this command on multiple sequences becomes easier because you can use wild card characters. For example, if you want to run fastqc and create reports that can be viewed, you can just type like this:\n",
    "\n",
    "```bash\n",
    "fastqc *.fastq\n",
    "```\n",
    "\n",
    "This means I am telling the `fastqc` tool to process any files in a given directory that ends with \".fastq\" extension. \n",
    "\n",
    "This is what you would see if you type the commands in your terminal. You can now inspect what files are being produced after the `fastqc` command was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 339952\n",
      "-rw-r--r--  1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_1.fastq\n",
      "-rw-r--r--@ 1 jimmysaw  staff   1.0M Jan 26 02:40 SRR12584454_1_fastqc.html\n",
      "-rw-r--r--  1 jimmysaw  staff   994K Jan 26 02:40 SRR12584454_1_fastqc.zip\n",
      "-rw-r--r--  1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_2.fastq\n",
      "-rw-r--r--  1 jimmysaw  staff   1.0M Jan 26 02:40 SRR12584454_2_fastqc.html\n",
      "-rw-r--r--  1 jimmysaw  staff   986K Jan 26 02:40 SRR12584454_2_fastqc.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/MicrobialGenomicsLab/exercises\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the command produced an \"html\" and a \"zip\" file for each fastq. Now, open the html files using your web browser. You should see something similar to this:\n",
    "\n",
    "![fastqc](images/fqc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a screenshot of the very beginning of the report. As you scroll down, you will see more information. Basically, the check marks under \"Summary\" will tell you whether the sequences are good or bad. If you see multiple red crosses, that means the sequences are of poor or bad quality and shouldn't be used right away until further processing is done. An example of a good and a bad sequencing run are shown here:\n",
    "\n",
    "https://www.bioinformatics.babraham.ac.uk/projects/fastqc/good_sequence_short_fastqc.html\n",
    "\n",
    "https://www.bioinformatics.babraham.ac.uk/projects/fastqc/bad_sequence_fastqc.html\n",
    "\n",
    "A few things to note about the histograms in the first plot. You can see that each sequence is 301 basepair long and the histogram is basically trying to depict average sequence quality within a given window of sequence. You will notice that average sequence quality starts to drop as you go towards the end of the sequence. This is due to the nature of Illumina sequencing technology. It uses fluorescent molecules to record unique bases (A, C, G, T), and the fluorescence signal fades towards the later cycles of sequencing. This makes it difficult to confidently assign correct letters of DNA towards the end and the instrument records lower read qualities near the end.\n",
    "\n",
    "Another thing you want to watch out for is the presence of \"adapter\" sequences (near the bottom of the report). See an example of bad sequence report. If the adapter sequences (which are artificial DNA constructs to facilitate sequencing) are left in the sequences for whatever reason, `fastqc` will detect it. In this bad sequence example, you will notice that the adapter content histogram starts to increase near the end of the sequences. If you see something like this, you will need to remove the adapter sequences before the sequences can be used in genome assemblies or other analyses.\n",
    "\n",
    "This is where the `bbmap` tool comes in.\n",
    "\n",
    "A very useful website on how to read and interpret sequencing quality assessments: https://sequencing.qcfail.com/articles/?report=reader\n",
    "\n",
    "#### Note to Windows OS users:\n",
    "\n",
    "You may have to figure out where your files are in Ubuntu environment. It is not straightforward to find out where the Ubuntu directory structure resides within the Windows environment. The straightforward way to make sure you are finding the right files in the location you are expecting to see, I would recommend creating a `workspace` folder in your C Drive. To get there from your Ubuntu environment, I would recommend you type this every time you start your Ubuntu terminal:\n",
    "\n",
    "```\n",
    "cd /mnt/c/\n",
    "```\n",
    "\n",
    "This will take you to the very base directory of your hard drive. See if you create a folder named `workspace` by typing `mkdir workspace`. If you can do this, then create a directory structure as I have and download the files there. Only then, you will be able to view the html output files using your Windows file browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BBTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBTools is a collection of tools written by scientists at the Joint Genome Institute (JGI) in Berkeley, CA. See here: https://jgi.doe.gov/data-and-tools/bbtools/\n",
    "\n",
    "It contains a number of tools that can perform tasks such as interconversion of file formats, processing of raw sequencing files into usable ones, mapping of sequence reads to reference genomes, etc. The tool we will be using today is `bbduk`, which is meant for filtering and trimming of reads for adapter, contaminants, and quality using k-mers. Before you can actually use this tool, you need a reference file containing all known adapter sequences. `bbduk` can then look up for these sequences to know if it can find these contaminants in your sequences.\n",
    "\n",
    "First, download this adapter file here:\n",
    "\n",
    "https://www.dropbox.com/s/f5mydteoupt8ugb/adapters.fa?dl=0\n",
    "\n",
    "I suggest you put this \"adapter.fa\" file somewhere safe where there is no likelihood of it being deleted by accident.\n",
    "\n",
    "Now, you can use `bbduk` to perform quality trimming and contaminant removal. To see what options are available with `bbduk`, type:\n",
    "\n",
    "`bbduk.sh -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Written by Brian Bushnell\n",
      "Last modified October 29, 2025\n",
      "\n",
      "Description:  Compares reads to the kmers in a reference dataset, optionally \n",
      "allowing an edit distance. Splits the reads into two outputs - those that \n",
      "match the reference, and those that don't. Can also trim (remove) the matching \n",
      "parts of the reads rather than binning the reads.\n",
      "Please read bbmap/docs/guides/BBDukGuide.txt for more information.\n",
      "\n",
      "Usage:  bbduk.sh in=<input file> out=<output file> ref=<contaminant files>\n",
      "\n",
      "Input may be stdin or a fasta or fastq file, compressed or uncompressed.\n",
      "If you pipe via stdin/stdout, please include the file type; e.g. for gzipped \n",
      "fasta input, set in=stdin.fa.gz\n",
      "\n",
      "Input parameters:\n",
      "in=<file>           Main input. in=stdin.fq will pipe from stdin.\n",
      "in2=<file>          Input for 2nd read of pairs in a different file.\n",
      "ref=<file,file>     Comma-delimited list of reference files.\n",
      "                    In addition to filenames, you may also use the keywords:\n",
      "                    adapters, artifacts, phix, lambda, pjet, mtst, kapa\n",
      "literal=<seq,seq>   Comma-delimited list of literal reference sequences.\n",
      "                    Polymers are also allowed with the 'poly' prefix;\n",
      "                    for example, 'literal=ATGGT,polyGC' will add both ATGGT\n",
      "                    and GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC - 32+ of them,\n",
      "                    enough replicates to ensure that all kmers are present.\n",
      "touppercase=f       (tuc) Change all bases upper-case.\n",
      "interleaved=auto    (int) t/f overrides interleaved autodetection.\n",
      "                    Must be set mainually when streaming fastq input.\n",
      "qin=auto            Input quality offset: 33 (Sanger), 64, or auto.\n",
      "reads=-1            If positive, quit after processing X reads or pairs.\n",
      "copyundefined=f     (cu) Process non-AGCT IUPAC reference bases by making all\n",
      "                    possible unambiguous copies.  Intended for short motifs\n",
      "                    or adapter barcodes, as time/memory use is exponential.\n",
      "samplerate=1        Set lower to only process a fraction of input reads.\n",
      "samref=<file>       Optional reference fasta for processing sam files.\n",
      "\n",
      "Output parameters:\n",
      "out=<file>          (outnonmatch) Write reads here that do not contain \n",
      "                    kmers matching the database.  'out=stdout.fq' will pipe \n",
      "                    to standard out.\n",
      "out2=<file>         (outnonmatch2) Use this to write 2nd read of pairs to a \n",
      "                    different file.\n",
      "outm=<file>         (outmatch) Write reads here that fail filters.  In default\n",
      "                    kfilter mode, this means any read with a matching kmer.\n",
      "                    In any mode, it also includes reads that fail filters such\n",
      "                    as minlength, mingc, maxgc, entropy, etc.  In other words,\n",
      "                    it includes all reads that do not go to 'out'.\n",
      "outm2=<file>        (outmatch2) Use this to write 2nd read of pairs to a \n",
      "                    different file.\n",
      "outs=<file>         (outsingle) Use this to write singleton reads whose mate \n",
      "                    was trimmed shorter than minlen.\n",
      "stats=<file>        Write statistics about which contamininants were detected.\n",
      "refstats=<file>     Write statistics on a per-reference-file basis.\n",
      "rpkm=<file>         Write RPKM for each reference sequence (for RNA-seq).\n",
      "dump=<file>         Dump kmer tables to a file, in fasta format.\n",
      "duk=<file>          Write statistics in duk's format. *DEPRECATED*\n",
      "nzo=t               Only write statistics about ref sequences with nonzero hits.\n",
      "overwrite=t         (ow) Grant permission to overwrite files.\n",
      "showspeed=t         (ss) 'f' suppresses display of processing speed.\n",
      "ziplevel=2          (zl) Compression level; 1 (min) through 9 (max).\n",
      "fastawrap=70        Length of lines in fasta output.\n",
      "qout=auto           Output quality offset: 33 (Sanger), 64, or auto.\n",
      "statscolumns=3      (cols) Number of columns for stats output, 3 or 5.\n",
      "                    5 includes base counts.\n",
      "rename=f            Rename reads to indicate which sequences they matched.\n",
      "refnames=f          Use names of reference files rather than scaffold IDs.\n",
      "trd=f               Truncate read and ref names at the first whitespace.\n",
      "ordered=f           Set to true to output reads in same order as input.\n",
      "maxbasesout=-1      If positive, quit after writing approximately this many\n",
      "                    bases to out (outu/outnonmatch).\n",
      "maxbasesoutm=-1     If positive, quit after writing approximately this many\n",
      "                    bases to outm (outmatch).\n",
      "json=f              Print to screen in json format.\n",
      "\n",
      "Histogram output parameters:\n",
      "bhist=<file>        Base composition histogram by position.\n",
      "qhist=<file>        Quality histogram by position.\n",
      "qchist=<file>       Count of bases with each quality value.\n",
      "aqhist=<file>       Histogram of average read quality.\n",
      "bqhist=<file>       Quality histogram designed for box plots.\n",
      "lhist=<file>        Read length histogram.\n",
      "phist=<file>        Polymer length histogram.\n",
      "gchist=<file>       Read GC content histogram.\n",
      "enthist=<file>      Read entropy histogram.\n",
      "ihist=<file>        Insert size histogram, for paired reads in mapped sam.\n",
      "gcbins=100          Number gchist bins.  Set to 'auto' to use read length.\n",
      "maxhistlen=6000     Set an upper bound for histogram lengths; higher uses \n",
      "                    more memory.  The default is 6000 for some histograms\n",
      "                    and 80000 for others.\n",
      "\n",
      "Histogram parameters for mapped sam/bam files only:\n",
      "histbefore=t        Calculate histograms from reads before processing.\n",
      "ehist=<file>        Errors-per-read histogram.\n",
      "qahist=<file>       Quality accuracy histogram of error rates versus quality \n",
      "                    score.\n",
      "indelhist=<file>    Indel length histogram.\n",
      "mhist=<file>        Histogram of match, sub, del, and ins rates by position.\n",
      "idhist=<file>       Histogram of read count versus percent identity.\n",
      "idbins=100          Number idhist bins.  Set to 'auto' to use read length.\n",
      "varfile=<file>      Ignore substitution errors listed in this file when \n",
      "                    calculating error rates.  Can be generated with\n",
      "                    CallVariants.\n",
      "vcf=<file>          Ignore substitution errors listed in this VCF file \n",
      "                    when calculating error rates.\n",
      "ignorevcfindels=t   Also ignore indels listed in the VCF.\n",
      "\n",
      "Processing parameters:\n",
      "k=31                Kmer length used for finding contaminants.  Contaminants \n",
      "                    shorter than k will not be found.  k must be at least 1.\n",
      "rcomp=t             Look for reverse-complements of kmers in addition to \n",
      "                    forward kmers.\n",
      "maskmiddle=t        (mm) Treat the middle base of a kmer as a wildcard, to \n",
      "                    increase sensitivity in the presence of errors.  This may\n",
      "                    also be set to a number, e.g. mm=3, to mask that many bp.\n",
      "                    The default mm=t corresponds to mm=1 for odd-length kmers\n",
      "                    and mm=2 for even-length kmers (as of v39.04), while\n",
      "                    mm=f is always equivalent to mm=0.\n",
      "minkmerhits=1       (mkh) Reads need at least this many matching kmers \n",
      "                    to be considered as matching the reference.\n",
      "minkmerfraction=0.0 (mkf) A reads needs at least this fraction of its total\n",
      "                    kmers to hit a ref, in order to be considered a match.\n",
      "                    If this and minkmerhits are set, the greater is used.\n",
      "mincovfraction=0.0  (mcf) A reads needs at least this fraction of its total\n",
      "                    bases to be covered by ref kmers to be considered a match.\n",
      "                    If specified, mcf overrides mkh and mkf.\n",
      "hammingdistance=0   (hdist) Maximum Hamming distance for ref kmers (subs only).\n",
      "                    Memory use is proportional to (3*K)^hdist.\n",
      "qhdist=0            Hamming distance for query kmers; impacts speed, not memory.\n",
      "editdistance=0      (edist) Maximum edit distance from ref kmers (subs \n",
      "                    and indels).  Memory use is proportional to (8*K)^edist.\n",
      "hammingdistance2=0  (hdist2) Sets hdist for short kmers, when using mink.\n",
      "qhdist2=0           Sets qhdist for short kmers, when using mink.\n",
      "editdistance2=0     (edist2) Sets edist for short kmers, when using mink.\n",
      "forbidn=f           (fn) Forbids matching of read kmers containing N.\n",
      "                    By default, these will match a reference 'A' if \n",
      "                    hdist>0 or edist>0, to increase sensitivity.\n",
      "removeifeitherbad=t (rieb) Paired reads get sent to 'outmatch' if either is \n",
      "                    match (or either is trimmed shorter than minlen).  \n",
      "                    Set to false to require both.\n",
      "trimfailures=f      Instead of discarding failed reads, trim them to 1bp.\n",
      "                    This makes the statistics a bit odd.\n",
      "findbestmatch=f     (fbm) If multiple matches, associate read with sequence \n",
      "                    sharing most kmers.  Reduces speed.\n",
      "skipr1=f            Don't do kmer-based operations on read 1.\n",
      "skipr2=f            Don't do kmer-based operations on read 2.\n",
      "ecco=f              For overlapping paired reads only.  Performs error-\n",
      "                    correction with BBMerge prior to kmer operations.\n",
      "recalibrate=f       (recal) Recalibrate quality scores.  Requires calibration\n",
      "                    matrices generated by CalcTrueQuality.\n",
      "sam=<file,file>     If recalibration is desired, and matrices have not already\n",
      "                    been generated, BBDuk will create them from the sam file.\n",
      "amino=f             Run in amino acid mode.  Some features have not been\n",
      "                    tested, but kmer-matching works fine.  Maximum k is 12.\n",
      "\n",
      "Speed and Memory parameters:\n",
      "threads=auto        (t) Set number of threads to use; default is number of \n",
      "                    logical processors.\n",
      "prealloc=f          Preallocate memory in table.  Allows faster table loading \n",
      "                    and more efficient memory usage, for a large reference.\n",
      "monitor=f           Kill this process if it crashes.  monitor=600,0.01 would \n",
      "                    kill after 600 seconds under 1% usage.\n",
      "minrskip=1          (mns) Force minimal skip interval when indexing reference \n",
      "                    kmers.  1 means use all, 2 means use every other kmer, etc.\n",
      "maxrskip=1          (mxs) Restrict maximal skip interval when indexing \n",
      "                    reference kmers. Normally all are used for scaffolds<100kb, \n",
      "                    but with longer scaffolds, up to maxrskip-1 are skipped.\n",
      "rskip=              Set both minrskip and maxrskip to the same value.\n",
      "                    If not set, rskip will vary based on sequence length.\n",
      "qskip=1             Skip query kmers to increase speed.  1 means use all.\n",
      "speed=0             Ignore this fraction of kmer space (0-15 out of 16) in both\n",
      "                    reads and reference.  Increases speed and reduces memory.\n",
      "Note: Do not use more than one of 'speed', 'qskip', and 'rskip'.\n",
      "\n",
      "Trimming/Filtering/Masking parameters:\n",
      "Note - if ktrim, kmask, and ksplit are unset, the default behavior is kfilter.\n",
      "All kmer processing modes are mutually exclusive.\n",
      "Reads only get sent to 'outm' purely based on kmer matches in kfilter mode.\n",
      "\n",
      "ktrim=f             Trim reads to remove bases matching reference kmers, plus\n",
      "                    all bases to the left or right.\n",
      "                    Values:\n",
      "                       f (don't trim), \n",
      "                       r (trim to the right), \n",
      "                       l (trim to the left)\n",
      "ktrimtips=0         Set this to a positive number to perform ktrim on both\n",
      "                    ends, examining only the outermost X bases.\n",
      "kmask=              Replace bases matching ref kmers with another symbol.\n",
      "                    Allows any non-whitespace character, and processes short\n",
      "                    kmers on both ends if mink is set.  'kmask=lc' will\n",
      "                    convert masked bases to lowercase.\n",
      "maskfullycovered=f  (mfc) Only mask bases that are fully covered by kmers.\n",
      "ksplit=f            For single-ended reads only.  Reads will be split into\n",
      "                    pairs around the kmer.  If the kmer is at the end of the\n",
      "                    read, it will be trimmed instead.  Singletons will go to\n",
      "                    out, and pairs will go to outm.  Do not use ksplit with\n",
      "                    other operations such as quality-trimming or filtering.\n",
      "mink=0              Look for shorter kmers at read tips down to this length, \n",
      "                    when k-trimming or masking.  0 means disabled.  Enabling\n",
      "                    this will disable maskmiddle.\n",
      "qtrim=f             Trim read ends to remove bases with quality below trimq.\n",
      "                    Performed AFTER looking for kmers.  Values: \n",
      "                       rl (trim both ends), \n",
      "                       f (neither end), \n",
      "                       r (right end only), \n",
      "                       l (left end only),\n",
      "                       w (sliding window).\n",
      "trimq=6             Regions with average quality BELOW this will be trimmed,\n",
      "                    if qtrim is set to something other than f.  Can be a \n",
      "                    floating-point number like 7.3.\n",
      "quantize            Bin quality scores to reduce file size.  quantize=2 will\n",
      "                    eliminate all odd quality scores, while quantize=0,10,37\n",
      "                    will only allow qualty scores of 0, 10, or 37.\n",
      "trimclip=f          Trim soft-clipped bases from sam files.\n",
      "minlength=10        (ml) Reads shorter than this after trimming will be \n",
      "                    discarded.  Pairs will be discarded if both are shorter.\n",
      "mlf=0               (minlengthfraction) Reads shorter than this fraction of \n",
      "                    original length after trimming will be discarded.\n",
      "maxlength=          Reads longer than this after trimming will be discarded.\n",
      "minavgquality=0     (maq) Reads with average quality (after trimming) below \n",
      "                    this will be discarded.\n",
      "maqb=0              If positive, calculate maq from this many initial bases.\n",
      "minbasequality=0    (mbq) Reads with any base below this quality (after \n",
      "                    trimming) will be discarded.\n",
      "maxns=-1            If non-negative, reads with more Ns than this \n",
      "                    (after trimming) will be discarded.\n",
      "mcb=0               (minconsecutivebases) Discard reads without at least \n",
      "                    this many consecutive called bases.\n",
      "ottm=f              (outputtrimmedtomatch) Output reads trimmed to shorter \n",
      "                    than minlength to outm rather than discarding.\n",
      "tp=0                (trimpad) Trim this much extra around matching kmers.\n",
      "tbo=f               (trimbyoverlap) Trim adapters based on where paired \n",
      "                    reads overlap.\n",
      "strictoverlap=t     Adjust sensitivity for trimbyoverlap mode.\n",
      "minoverlap=14       Require this many bases of overlap for detection.\n",
      "mininsert=40        Require insert size of at least this for overlap.\n",
      "                    Should be reduced to 16 for small RNA sequencing.\n",
      "tpe=f               (trimpairsevenly) When kmer right-trimming, trim both \n",
      "                    reads to the minimum length of either.\n",
      "forcetrimleft=0     (ftl) If positive, trim bases to the left of this position\n",
      "                    (exclusive, 0-based).\n",
      "forcetrimright=0    (ftr) If positive, trim bases to the right of this position\n",
      "                    (exclusive, 0-based).\n",
      "forcetrimright2=0   (ftr2) If positive, trim this many bases on the right end.\n",
      "forcetrimmod=0      (ftm) If positive, right-trim length to be equal to zero,\n",
      "                    modulo this number.\n",
      "restrictleft=0      If positive, only look for kmer matches in the \n",
      "                    leftmost X bases.\n",
      "restrictright=0     If positive, only look for kmer matches in the \n",
      "                    rightmost X bases.\n",
      "NOTE:  restrictleft and restrictright are mutually exclusive.  If trimming\n",
      "       both ends is desired, use ktrimtips.\n",
      "mingc=0             Discard reads with GC content below this.\n",
      "maxgc=1             Discard reads with GC content above this.\n",
      "gcpairs=t           Use average GC of paired reads.\n",
      "                    Also affects gchist.\n",
      "tossjunk=f          Discard reads with invalid characters as bases.\n",
      "swift=f             Trim Swift sequences: Trailing C/T/N R1, leading G/A/N R2.\n",
      "\n",
      "Header-parsing parameters - these require Illumina headers:\n",
      "chastityfilter=f    (cf) Discard reads with id containing ' 1:Y:' or ' 2:Y:'.\n",
      "barcodefilter=f     Remove reads with unexpected barcodes if barcodes is set,\n",
      "                    or barcodes containing 'N' otherwise.  A barcode must be\n",
      "                    the last part of the read header.  Values:\n",
      "                       t:     Remove reads with bad barcodes.\n",
      "                       f:     Ignore barcodes.\n",
      "                       crash: Crash upon encountering bad barcodes.\n",
      "barcodes=           Comma-delimited list of barcodes or files of barcodes.\n",
      "xmin=-1             If positive, discard reads with a lesser X coordinate.\n",
      "ymin=-1             If positive, discard reads with a lesser Y coordinate.\n",
      "xmax=-1             If positive, discard reads with a greater X coordinate.\n",
      "ymax=-1             If positive, discard reads with a greater Y coordinate.\n",
      "\n",
      "Polymer trimming parameters:\n",
      "trimpolya=0         If greater than 0, trim poly-A or poly-T tails of\n",
      "                    at least this length on either end of reads.\n",
      "trimpolygleft=0     If greater than 0, trim poly-G prefixes of at least this\n",
      "                    length on the left end of reads.  Does not trim poly-C.\n",
      "trimpolygright=0    If greater than 0, trim poly-G tails of at least this \n",
      "                    length on the right end of reads.  Does not trim poly-C.\n",
      "trimpolyg=0         This sets both left and right at once.\n",
      "filterpolyg=0       If greater than 0, remove reads with a poly-G prefix of\n",
      "                    at least this length (on the left).\n",
      "Note: there are also equivalent poly-C flags.\n",
      "\n",
      "Polymer tracking parameters:\n",
      "pratio=base,base    'pratio=G,C' will print the ratio of G to C polymers.\n",
      "plen=20             Length of homopolymers to count.\n",
      "\n",
      "Entropy/Complexity parameters:\n",
      "entropy=-1          Set between 0 and 1 to filter reads with entropy below\n",
      "                    that value.  Higher is more stringent.\n",
      "entropywindow=50    Calculate entropy using a sliding window of this length.\n",
      "entropyk=5          Calculate entropy using kmers of this length.\n",
      "minbasefrequency=0  Discard reads with a minimum base frequency below this.\n",
      "entropytrim=f       Values:\n",
      "                       f:  (false) Do not entropy-trim.\n",
      "                       r:  (right) Trim low entropy on the right end only.\n",
      "                       l:  (left) Trim low entropy on the left end only.\n",
      "                       rl: (both) Trim low entropy on both ends.\n",
      "entropymask=f       Values:\n",
      "                       f:  (filter) Discard low-entropy sequences.\n",
      "                       t:  (true) Mask low-entropy parts of sequences with N.\n",
      "                       lc: Change low-entropy parts of sequences to lowercase.\n",
      "entropymark=f       Mark each base with its entropy value.  This is on a scale\n",
      "                    of 0-41 and is reported as quality scores, so the output\n",
      "                    should be fastq or fasta+qual.\n",
      "NOTE: If set, entropytrim overrides entropymask.\n",
      "\n",
      "Cardinality estimation parameters:\n",
      "cardinality=f       (loglog) Count unique kmers using the LogLog algorithm.\n",
      "cardinalityout=f    (loglogout) Count unique kmers in output reads.\n",
      "loglogk=31          Use this kmer length for counting.\n",
      "loglogbuckets=2048  Use this many buckets for counting.\n",
      "khist=<file>        Kmer frequency histogram; plots number of kmers versus\n",
      "                    kmer depth.  This is approximate.\n",
      "khistout=<file>     Kmer frequency histogram for output reads.\n",
      "\n",
      "Side Channel Parameters:\n",
      "sideout=<file>      Output for aligned reads.\n",
      "sideref=phix        Reference for side-channel alignment; must be a single\n",
      "                    sequence and virtually repeat-free at selected k.\n",
      "sidek1=17           Kmer length for seeding alignment to reference.\n",
      "sidek2=13           Kmer length for seeding alignment of unaligned reads\n",
      "                    with an aligned mate.\n",
      "sideminid1=0.66     Minimum identity to accept individual alignments.\n",
      "sideminid2=0.58     Minimum identity for aligning reads with aligned mates.\n",
      "sidemm1=1           Middle mask length for sidek1.\n",
      "sidemm2=1           Middle mask length for sidek2.\n",
      "Note:  The side channel is a special additional output that allows alignment\n",
      "to a secondary reference while also doing trimming.  Alignment does not affect\n",
      "whether reads go to the normal outputs (out, outm).  The main purpose is to\n",
      "simplify pipelines that need trimmed, aligned phiX reads for recalibration.\n",
      "\n",
      "\n",
      "Java Parameters:\n",
      "\n",
      "-Xmx                This will set Java's memory usage, overriding autodetection.\n",
      "                    -Xmx20g will \n",
      "                    specify 20 gigs of RAM, and -Xmx200m will specify 200 megs.  \n",
      "                    The max is typically 85% of physical memory.\n",
      "-eoom               This flag will cause the process to exit if an \n",
      "                    out-of-memory exception occurs.  Requires Java 8u92+.\n",
      "-da                 Disable assertions.\n",
      "\n",
      "Please contact Brian Bushnell at bbushnell@lbl.gov if you encounter any problems.\n",
      "For documentation and the latest version, visit: https://bbmap.org\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bbduk.sh -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tool was written by Brian Bushnell and there are a lot of options you can specify with this tool. It can be pretty overwhelming to read and understand each of the parameter available for you to use. So I am giving you an example command with parameters I use for routing processing of raw sequence files.\n",
    "\n",
    "```bash\n",
    "bbduk.sh ktrim=r ordered minlen=50 mink=11 tbo rcomp=f k=21 ow=t ftm=5 zl=4 \\\n",
    "        qtrim=rl trimq=20 \\\n",
    "        in1=SRR12584454_1.fastq \\\n",
    "        in2=SRR12584454_2.fastq \\\n",
    "        ref=adapters.fa \\\n",
    "        out1=SRR12584454_1.trimmed.fastq \\\n",
    "        out2=SRR12584454_2.trimmed.fastq\n",
    "```\n",
    "\n",
    "Here you will notice a few things that are important in this example. First, I am telling `bbduk` to only keep sequences longer than 50 bases and quality higher than 20. And I am specifying the adapter file in `ref=adapters.fa` flag. `in1` and `in2` are where you specify input fastq files and `out1` and `out2` are where you specify output files, which should be named differently so that the program doesn't overwrite the original files. I will show what it looks like when you type this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected 16777216KB total memory on macOS, estimating 10905190KB available\n",
      "Detected 5633804KB free memory on macOS (vm_stat)\n",
      "Detected 16777216KB total memory on macOS, estimating 10905190KB available\n",
      "Detected 5633156KB free memory on macOS (vm_stat)\n",
      "java -ea   -Xmx2105m -Xms2105m -cp /Users/jimmysaw/miniconda3/envs/genomics/opt/bbmap-39.52-0/current/ jgi.BBDuk ktrim=r ordered minlen=50 mink=11 tbo rcomp=f k=21 ow=t ftm=5 zl=4 qtrim=rl trimq=20 in1=SRR12584454_1.fastq in2=SRR12584454_2.fastq ref=adapters.fa out1=SRR12584454_1.trimmed.fastq out2=SRR12584454_2.trimmed.fastq\n",
      "Executing jgi.BBDuk [ktrim=r, ordered, minlen=50, mink=11, tbo, rcomp=f, k=21, ow=t, ftm=5, zl=4, qtrim=rl, trimq=20, in1=SRR12584454_1.fastq, in2=SRR12584454_2.fastq, ref=adapters.fa, out1=SRR12584454_1.trimmed.fastq, out2=SRR12584454_2.trimmed.fastq]\n",
      "Version 39.52\n",
      "\n",
      "Set ORDERED to true\n",
      "maskMiddle was disabled because useShortKmers=true\n",
      "Allocating kmer table: \t0.020 seconds.\n",
      "Initial:\n",
      "Memory: max=2208m, total=2208m, free=2185m, used=23m\n",
      "\n",
      "Added 3225 kmers; time: \t0.030 seconds.\n",
      "Memory: max=2208m, total=2208m, free=2175m, used=33m\n",
      "\n",
      "Input is being processed as paired\n",
      "Started output streams:\t0.056 seconds.\n",
      "Processing time:   \t\t6.836 seconds.\n",
      "\n",
      "Input:                  \t207684 reads \t\t62512884 bases.\n",
      "QTrimmed:               \t155680 reads (74.96%) \t14257391 bases (22.81%)\n",
      "FTrimmed:               \t207684 reads (100.00%) \t207684 bases (0.33%)\n",
      "KTrimmed:               \t128 reads (0.06%) \t25083 bases (0.04%)\n",
      "Trimmed by overlap:     \t167802 reads (80.80%) \t1345557 bases (2.15%)\n",
      "Total Removed:          \t12800 reads (6.16%) \t15835715 bases (25.33%)\n",
      "Result:                 \t194884 reads (93.84%) \t46677169 bases (74.67%)\n",
      "\n",
      "Time:                         \t6.923 seconds.\n",
      "Reads Processed:        207k \t30.00k reads/sec\n",
      "Bases Processed:      62512k \t9.03m bases/sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/MicrobialGenomicsLab/exercises/\n",
    "bbduk.sh ktrim=r ordered minlen=50 mink=11 tbo rcomp=f k=21 ow=t ftm=5 zl=4 \\\n",
    "        qtrim=rl trimq=20 \\\n",
    "        in1=SRR12584454_1.fastq \\\n",
    "        in2=SRR12584454_2.fastq \\\n",
    "        ref=adapters.fa \\\n",
    "        out1=SRR12584454_1.trimmed.fastq \\\n",
    "        out2=SRR12584454_2.trimmed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the tool finishes this task (which is pretty fast), you will see that it found 207,684 reads (if you combine the 2 fastq files) and removed 12,800 reads that do not meet the criteria I have set. And it retained **93.84%** of the reads. The whole process only took less than **7 seconds!** Now, if you look into the folder, you will notice 2 new files being produced that end with \"`.trimmed.fastq`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 538656\n",
      "drwxr-xr-x  11 jimmysaw  staff   352B Jan 26 02:49 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x   6 jimmysaw  staff   192B Jan 26 02:26 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--   1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_1.fastq\n",
      "-rw-r--r--   1 jimmysaw  staff    53M Jan 26 02:50 SRR12584454_1.trimmed.fastq\n",
      "-rw-r--r--@  1 jimmysaw  staff   1.0M Jan 26 02:40 SRR12584454_1_fastqc.html\n",
      "-rw-r--r--   1 jimmysaw  staff   994K Jan 26 02:40 SRR12584454_1_fastqc.zip\n",
      "-rw-r--r--   1 jimmysaw  staff    67M Jan 26 02:27 SRR12584454_2.fastq\n",
      "-rw-r--r--   1 jimmysaw  staff    43M Jan 26 02:50 SRR12584454_2.trimmed.fastq\n",
      "-rw-r--r--   1 jimmysaw  staff   1.0M Jan 26 02:40 SRR12584454_2_fastqc.html\n",
      "-rw-r--r--   1 jimmysaw  staff   986K Jan 26 02:40 SRR12584454_2_fastqc.zip\n",
      "-rw-r--r--@  1 jimmysaw  staff    14K Jan 26 02:48 adapters.fa\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/MicrobialGenomicsLab/exercises/\n",
    "ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step after this is to check if sequence qualities have improved after you have run `bbduk` tool. To do that, you run `fastqc` tool again but on the trimmed fastq files. Type:\n",
    "\n",
    "```bash\n",
    "fastqc *.trimmed.fastq\n",
    "```\n",
    "\n",
    "And inspect the sequence qualities by opening the html files produced by `fastqc`. Noticed any differences? You can open the `fastqc` report of the original file in another tab to see how different things are. You should see something like this:\n",
    "\n",
    "![trimmed](images/fqct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice, the histogram denoting sequence quality have drastically improved (no more columns dipping into the red zone). Now, the sequences are good to be used in downstream processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparations for next week's lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing genome and metagenome assemblies next week on a remote computational cluster. For that, you will download a few publicly available datasets and also familiarize yourself with using `ssh` and `rsync` tools to remotely connect to and transfer files to other computers. You can look at this page for some examples of how to use `ssh`.\n",
    "\n",
    "https://phoenixnap.com/kb/linux-ssh-commands\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tool that will be crucial for you to use next week will be this tool known as `rsync`. It is a command line tool that can be used to transfer files between two remote computers or even for syncing directories inside the same computer. You can see some examples of how to use `rsync` here:\n",
    "\n",
    "https://www.tecmint.com/rsync-local-remote-file-synchronization-commands/\n",
    "\n",
    "and here:\n",
    "\n",
    "https://phoenixnap.com/kb/rsync-command-linux-examples\n",
    "\n",
    "Try to play around with a few examples shown in these pages to become familar with how `rsync` works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a ssh key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ssh key is something important for you to log on to the Cerberus teaching cluster owned by GW. First time you are logging on to this cluster, it will ask for GW username and password. But once you have created an ssh key on your local computer (laptop), you can copy a \"public\" ssh key onto the remote computer you're trying to access and next time you log on, you won't need to type the password at all and it will just let you in by cross-checking your laptop ssh key with what's on the remote computer.\n",
    "\n",
    "In order for you to be able to do that, you will have to email your **public** ssh key to the system administrator. I will let you know which email to send your public ssh key to. Today, focus on getting an ssh key created. Some examples are shown here (https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-2) but I will write down a few quick commands to get you there. Type the following in your computer's terminal.\n",
    "\n",
    "```\n",
    "ssh-keygen -t rsa\n",
    "```\n",
    "\n",
    "And follow the prompts. If you have successfully created your ssh key, the public key will have a name like this: `id_rsa.pub`\n",
    "\n",
    "Type this: `cat ~/.ssh/id_rsa.pub`\n",
    "\n",
    "This will print a bunch of characters on your terminal. Copy the whole thing printed and send it to the system administrator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data for next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using publicly available sequence data from NCBI SRA and see if you can replicate their results. The first one to download is here:\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/sra/SRX9094324\n",
    "\n",
    "Remember that the actuall accession of the raw sequencing files start with \"SRR\". Look for it in that page.\n",
    "\n",
    "These are Illumina MiSeq sequences they have used to reconstruct the genome of a *Salmonella enteria* strain. This is a cultured isolate and represents a single organism. Use `fasterq-dump` tool to download this data. I recommend you put it in `data` folder on your computer. These are fairly large files (each about 450 MB). In order to save space, you can compress them after they are downloaded. You can type like this:\n",
    "\n",
    "```bash\n",
    "gzip *.fastq\n",
    "```\n",
    "\n",
    "And this will compress your text files into much smaller zipped files with extension \"*.gz\". The file sizes are much smaller after being compressed (about 110-130 MB).\n",
    "\n",
    "For metagenome assemblies, we will be downloading this file:\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/sra/SRX4741377\n",
    "\n",
    "Make sure you use `fasterq-dump` to download it. Again, I recommend you put it in `data` folder on your computer and also use `gzip` to compress the fastq files as these are very large files (each about 4.5 GB, a total of 9 GB). These compressed files can be used directly by both `fastqc` and `bbduk`.\n",
    "\n",
    "Now you are ready for next week's lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
